{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working-With-Hugging-face_Pipelines\n",
    "=============================\n",
    "\n",
    "    *M-Zia-Rasa*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* **Hugging-face** is a leading company in the field of artificial intelligence, particularly known for its contributions to natural language processing (NLP). They have developed the Transformers library, which is one of the most popular libraries for NLP tasks. Here's an overview of what Hugging Face offers:\n",
    "\n",
    "    * **Pre-trained-Models**: Provides thousands of pre-trained models for NLP, vision, audio tasks and supports a wide range of architectures such as BERT, GPT, T5, and more.\n",
    "\n",
    "    * **Datasets**: A library that offers access to thousands of datasets across various domains. It provides tools to load, preprocess, and manipulate datasets efficiently.\n",
    "\n",
    "    * **Inference API**: Allows users to deploy models for inference without managing the infrastructure. It provides an easy way to integrate models into applications using simple API calls.\n",
    "\n",
    "    * **Hugging Face Spaces**: A service that allows developers to create and share AI applications powered by Gradio or Streamlit.\n",
    "\n",
    "\n",
    "* **Pipeline** is used to automate workflows,ensure that data flows smoothly through various states of processing. In NLP pipeline refers to to Pre-Processing, instantiating the model and Post-processing of the texts.\n",
    "\n",
    "    * **PreProcessing**: is used to process the raw-texts like tokenizing, removing numbers, etc. to create pure text.\n",
    "    * **Instantiating-Model**: is used to perform a specific task like text-generation, question-answering, etc.\n",
    "    * **Post-Processing** is used to to output a readable representation of the text.\n",
    "\n",
    "* Now we learn how to use pipeline to accomplish different tasks as follows:\n",
    "\n",
    "    * **Text-Classification**\n",
    "    * **Zero-Shot-Classification**\n",
    "    * **Token-Classification (Group-Entity)**\n",
    "    * **Text-Completion(Mask-Filling)** \n",
    "    * **Text-Generation**\n",
    "    * **Text-Summarization**\n",
    "    * **Question-Answering**\n",
    "\n",
    "* **What really happens inside a pipeline** ???\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing important packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\rasam\\anaconda3\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from transformers) (0.26.5)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\rasam\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in c:\\users\\rasam\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\rasam\\anaconda3\\lib\\site-packages (0.26.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from huggingface-hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from huggingface-hub) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from huggingface-hub) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from huggingface-hub) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from huggingface-hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from huggingface-hub) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from huggingface-hub) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\rasam\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.42.1->huggingface-hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from requests->huggingface-hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from requests->huggingface-hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from requests->huggingface-hub) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from requests->huggingface-hub) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tf-keras in c:\\users\\rasam\\anaconda3\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\rasam\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.68.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.7.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\rasam\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers\n",
    "%pip install torch\n",
    "%pip install huggingface-hub\n",
    "%pip install tf-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([{'label': 'POSITIVE', 'score': 0.9991065859794617}],)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline (\"sentiment-analysis\")\n",
    "classifier (\"I have been waiting for hugging-face course in my entire life\"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9932845830917358},\n",
       " {'label': 'POSITIVE', 'score': 0.9998409748077393},\n",
       " {'label': 'NEGATIVE', 'score': 0.9934645295143127},\n",
       " {'label': 'NEGATIVE', 'score': 0.9972587823867798}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline (\"sentiment-analysis\")\n",
    "classifier ([\n",
    "    \"I have been waiting for huggingface course in my entire life\",\n",
    "    \"I really love these glasses\",\n",
    "    \"I don't want to use these sunglasses\",\n",
    "    \"A few people go to the movies nowadays because it is boring somehow\"\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot-Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'I have been waiting for huggingface course in my entire life',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.4752252995967865, 0.27590247988700867, 0.2488723248243332]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline (\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"I have been waiting for huggingface course in my entire life\",\n",
    "    candidate_labels = [\"business\", \"education\", \"politics\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': \"Elon Mask's neat benefits is four hundred million dollars\",\n",
       "  'labels': ['business', 'politics', 'education'],\n",
       "  'scores': [0.9523494839668274, 0.03159298002719879, 0.01605755090713501]},\n",
       " {'sequence': 'Learning English very important nowadays',\n",
       "  'labels': ['education', 'business', 'politics'],\n",
       "  'scores': [0.9403640627861023, 0.05317789688706398, 0.006458061747252941]},\n",
       " {'sequence': 'President Jue Biden was killed yesterday',\n",
       "  'labels': ['politics', 'business', 'education'],\n",
       "  'scores': [0.9040516018867493, 0.07048234343528748, 0.025466060265898705]}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline (\"zero-shot-classification\")\n",
    "classifier([\n",
    "    \"Elon Mask's neat benefits is four hundred million dollars\",\n",
    "    \"Learning English very important nowadays\",\n",
    "    \"President Jue Biden was killed yesterday\"\n",
    "    ],\n",
    "\n",
    "    candidate_labels = [\"business\", \"education\", \"politics\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token-Classification (Group-Entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\rasam\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.99941146,\n",
       "  'word': 'John Smith',\n",
       "  'start': 0,\n",
       "  'end': 10},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9937976,\n",
       "  'word': 'Microsoft',\n",
       "  'start': 27,\n",
       "  'end': 36}]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "entity = pipeline(\"ner\", grouped_entities=True)\n",
    "\n",
    "entity (\"John Smith is working in a Microsoft in Untied State\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-Completion(Mask-Filling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.06788498163223267,\n",
       "  'token': 2788,\n",
       "  'token_str': ' text',\n",
       "  'sequence': 'There are many open-source text models available for text-gner'},\n",
       " {'score': 0.02523723430931568,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'There are many open-source mathematical models available for text-gner'},\n",
       " {'score': 0.019201966002583504,\n",
       "  'token': 41806,\n",
       "  'token_str': ' graphical',\n",
       "  'sequence': 'There are many open-source graphical models available for text-gner'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "mask = pipeline(\"fill-mask\")\n",
    "\n",
    "mask(\"There are many open-source <mask> models available for text-gner\", top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"In this course, we will teach you how to build the network, how to use your data to create web apps, and how to create your own websites using Python. We will use these resources to create content that's fast, easy, and beautiful\"}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\")\n",
    "\n",
    "generator(\"In this course, we will teach you how to\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to successfully use the Google Play Store and Google Play Store on-time for more advanced, mobile app features, and'},\n",
       " {'generated_text': \"In this course, we will teach you how to create one of the top videos you will find online. If you don't have the material, don't hesitate\"}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\" , model=\"distilgpt2\")\n",
    "\n",
    "generator(\"In this course, we will teach you how to\",\n",
    "          max_length=32,\n",
    "          num_return_sequences=2,\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' A Hugging Face pipeline is a predefined sequence of tasks that can be applied to text, audio, vision, or multi-modal inputs . Pipelines are designed for specific tasks such as text classification, sentiment analysis, image classification, and more . They simplify the process of applying complex models to real-world data .'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "tex = pipeline(\"summarization\")\n",
    "\n",
    "tex (\n",
    "    \n",
    "    \"\"\" \n",
    "    A Hugging Face pipeline is a predefined sequence of tasks that can be applied to text, audio, vision, or multi-modal inputs.\n",
    "    These pipelines are part of the Hugging Face Transformers library, which provides a wide range of pre-trained models and tools for natural language processing (NLP).\n",
    "    \n",
    "    Key Features of Hugging Face Pipelines\n",
    "        Predefined Tasks: Pipelines are designed for specific tasks such as text classification, sentiment analysis, image classification, and more. They simplify the process of applying complex models to real-world data1.\n",
    "        Ease of Use: Pipelines abstract away much of the complexity involved in setting up and running models. Users can easily load a pipeline and start using it with minimal setup1.\n",
    "        Modularity: Pipelines are modular, meaning you can easily swap out different models or components as needed.\n",
    "        Integration: They integrate seamlessly with other Hugging Face tools and libraries, making it easy to build end-to-end workflows.\n",
    "        \"\"\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question-Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9543827176094055, 'start': 10, 'end': 19, 'answer': 'Microsoft'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "question_answering = pipeline(\"question-answering\")\n",
    "\n",
    "question_answering(\n",
    "    question=\"Where do you work?\",\n",
    "    context=\"I work in Microsoft.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** What really happens inside a pipeline?? **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what happens inside a pipeline and how a pipeline handles and automate the workflows of all the processing stages like:\n",
    "\n",
    "    * **Pre-processing**\n",
    "    * **Model-Instantiating**\n",
    "    * **Post-Processing**\n",
    "\n",
    "Here, we consider the below example of **sentiment-analysis** pipeline and expands it how it works through all the stages step by step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.998216450214386},\n",
       " {'label': 'NEGATIVE', 'score': 0.9995666146278381}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline (\"sentiment-analysis\")\n",
    "classifier ([\n",
    "    \"She is very interested in learning machine learning with hugging face.\",\n",
    "    \"She hates learning English language\"   \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:\n",
    "\n",
    "**Pre-Processing** is used to process the raw-texts to create pure text for a model input to be understanding for the model it uses a technique called tokenizing.\n",
    "\n",
    "    **Tokenizing** is used to break down the texts into smaller units called tokens. These tokens can be words, sub-words, or characters, depending on the specific tokenization approach and then covert each token into numeric presentation to be easier for machine learning models to process and analyze the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens for text1: {'input_ids': tensor([[  101,  2016,  2003,  2200,  4699,  1999,  4083,  3698,  4083,  2007,\n",
      "         17662,  2227,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "Tokens for text2: {'input_ids': tensor([[  101,  2016, 16424,  4083,  2394,  2653,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_model = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
    "\n",
    "# Example texts\n",
    "text1 = \"She is very interested in learning machine learning with hugging face.\" \n",
    "text2 = \"She hates learning English language\"\n",
    "\n",
    "\n",
    "inputs1 = tokenizer (text1, padding=True, truncation=True, return_tensors='pt')\n",
    "inputs2 = tokenizer (text2, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "print(f\"Tokens for text1: {inputs1}\")\n",
    "print(f\"Tokens for text2: {inputs2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:\n",
    "\n",
    "**Model-Instantiating**:In this step: After the raw_text is processed, it is given to large language model or any fine-tuned model to perform a specific task like sentiment-analysis, text-generation, question-answering, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model instantiating with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 14, 768])\n",
      "torch.Size([1, 7, 768])\n",
      "Model outputs for text1: BaseModelOutput(last_hidden_state=tensor([[[ 0.1052,  0.1038,  0.4744,  ...,  0.2579,  0.7159, -0.3149],\n",
      "         [ 0.3117, -0.0781,  0.2464,  ...,  0.2481,  0.6948, -0.4315],\n",
      "         [ 0.2061,  0.0306,  0.3996,  ...,  0.2917,  0.7380, -0.2845],\n",
      "         ...,\n",
      "         [ 0.3097,  0.1896,  0.7819,  ...,  0.1903,  0.7685, -0.0713],\n",
      "         [ 0.2798,  0.2286,  0.6623,  ...,  0.5312,  0.6744, -0.5555],\n",
      "         [ 0.5922,  0.4073,  0.4609,  ...,  0.6287,  0.5652, -0.7413]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)\n",
      "Model outputs for text2: BaseModelOutput(last_hidden_state=tensor([[[-0.3099,  0.6556, -0.2782,  ...,  0.0112, -1.0256, -0.2191],\n",
      "         [-0.3476,  0.4199, -0.4006,  ...,  0.0151, -0.7378, -0.2960],\n",
      "         [-0.4644,  0.8465, -0.1417,  ..., -0.0858, -0.9849, -0.2328],\n",
      "         ...,\n",
      "         [-1.1499,  0.8378, -0.6173,  ..., -0.3606, -0.2767,  0.0499],\n",
      "         [-0.6680,  0.3437, -0.5069,  ..., -0.1604, -0.0724, -0.0083],\n",
      "         [-0.0598,  0.8124, -0.4702,  ..., -0.0270, -0.8069, -0.3373]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "llm_model = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(llm_model)\n",
    "\n",
    "outputs1 = model(**inputs1)\n",
    "outputs2 = model(**inputs2)\n",
    "\n",
    "print(outputs1.last_hidden_state.shape)\n",
    "print(outputs2.last_hidden_state.shape)\n",
    "\n",
    "print(f\"Model outputs for text1: {outputs1}\") \n",
    "print(f\"Model outputs for text2: {outputs2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.0720,  3.2554]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 4.3080, -3.4354]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "llm_model = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(llm_model)\n",
    "\n",
    "outputs1 = model(**inputs1)\n",
    "outputs2 = model(**inputs2)\n",
    "\n",
    "print(outputs1.logits)\n",
    "print(outputs2.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3:\n",
    "\n",
    "**Post-Processing**: is used to to output a readable representation of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0018, 0.9982]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[9.9957e-01, 4.3341e-04]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "result1 = torch.nn.functional.softmax(outputs1.logits, dim=-1)\n",
    "print(result1)\n",
    "\n",
    "result2 = torch.nn.functional.softmax(outputs2.logits, dim=-1)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: 'She is very interested in learning machine learning with hugging face.'\n",
      "Sentiment: positive, Score: 0.9982\n",
      "\n",
      "Text 2: 'She hates learning English language'\n",
      "Sentiment: negative, Score: 0.9996\n"
     ]
    }
   ],
   "source": [
    "# Get the labels and scores \n",
    "labels = [\"negative\", \"positive\"] \n",
    "label1 = labels[torch.argmax(result1).item()] \n",
    "label2 = labels[torch.argmax(result2).item()] \n",
    "score1 = result1[0][torch.argmax(result1)].item() \n",
    "score2 = result2[0][torch.argmax(result2)].item() \n",
    "\n",
    "# Print the results \n",
    "print(f\"Text 1: '{text1}'\") \n",
    "print(f\"Sentiment: {label1}, Score: {score1:.4f}\\n\") \n",
    "print(f\"Text 2: '{text2}'\") \n",
    "print(f\"Sentiment: {label2}, Score: {score2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment-Analysis with more brief code with the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: She is very interested in learning machine learning with hugging face.\n",
      "Sentiment: POSITIVE, Score: 0.9982\n",
      "\n",
      "Text: She hates learning English language\n",
      "Sentiment: NEGATIVE, Score: 0.9996\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "# Tokenizing\n",
    "tokenizer_model = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_model)\n",
    "raw_text = [\n",
    "    \"She is very interested in learning machine learning with hugging face.\",\n",
    "    \"She hates learning English language\"\n",
    "]\n",
    "inputs = tokenizer(raw_text, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Step 2: Model Use\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Step 3: Post-processing\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "results = sentiment_analyzer(raw_text)\n",
    "\n",
    "# Print the results\n",
    "for text, result in zip(raw_text, results):\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {result['label']}, Score: {result['score']:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment-Analysis with Huggingface Pipeline with same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.998216450214386},\n",
       " {'label': 'NEGATIVE', 'score': 0.9995666146278381}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline (\"sentiment-analysis\")\n",
    "classifier ([\n",
    "    \"She is very interested in learning machine learning with hugging face.\",\n",
    "    \"She hates learning English language\"   \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 😍😍 Thanks to the hugging face of pipeline that how made the the coding easier ❤️❤️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank You!\n",
    "\n",
    "Please contribute on my github repository https://github.com/m-zia-rasa/NLP-Hugging_face-Pipelines.git"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
